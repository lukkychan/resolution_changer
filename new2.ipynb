{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukkychan/resolution_changer/blob/main/new2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!rm -rf frames\n",
        "!rm -rf frames/720p\n",
        "!rm -rf frames/360p\n",
        "!rm -rf scaled\n",
        "!rm -rf predicted\n",
        "!rm -rf Pre_orginal\n",
        "!rm -rf dataset\n",
        "!rm -rf dataset/720p\n",
        "!rm -rf dataset/360s\n",
        "!rm -rf checkpoints\n",
        "!rm -rf temp_model\n",
        "!rm -rf temp_scaled\n",
        ""
      ],
      "metadata": {
        "id": "m7LhdpJC_Qj3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OSMpQkNP4QQz",
        "outputId": "92c08cd4-9209-4ce8-f113-8496d45424cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p frames/720p\n",
        "!mkdir -p frames/360p\n",
        "!mkdir scaled\n",
        "!mkdir predicted\n",
        "!mkdir dataset\n",
        "!mkdir -p dataset/720p\n",
        "!mkdir -p dataset/360p\n",
        "!mkdir checkpoints"
      ],
      "metadata": {
        "id": "3uZ7cqmn4Rla"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Specify the path to the video file\n",
        "video_path = \"/content/drive/MyDrive/model/1080p.mp4\"\n",
        "\n",
        "# Specify the output folder to save the frames\n",
        "output_folder = \"/content/frames/360p/\"\n",
        "\n",
        "# Open the video file\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get the total number of frames in the video\n",
        "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "print(total_frames)\n",
        "\n",
        "# Calculate the frame interval to evenly sample the frames\n",
        "frame_interval = max(total_frames//125, 1)\n",
        "\n",
        "# Initialize a counter to keep track of the extracted frames\n",
        "frame_count_1 = 0\n",
        "\n",
        "# Loop through the frames and extract the desired number of frames\n",
        "while frame_count_1 <= 125 :\n",
        "    # Read the current frame\n",
        "    ret, frame = video.read()\n",
        "\n",
        "    # Check if the frame was successfully read\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Save the frame as an image file\n",
        "    frame_path = f\"{output_folder}{frame_count_1}.jpg\"\n",
        "    cv2.imwrite(frame_path, frame)\n",
        "\n",
        "    # Increment the frame count\n",
        "    frame_count_1 += 1\n",
        "\n",
        "    # Move to the next frame based on the frame interval\n",
        "    video.set(cv2.CAP_PROP_POS_FRAMES, frame_count_1 * frame_interval)\n",
        "\n",
        "# Release the video capture object\n",
        "video.release()\n",
        "print(frame_count_1)\n",
        "print(\"done.\")\n"
      ],
      "metadata": {
        "id": "p4YhYKAr4TPZ",
        "outputId": "230894de-2224-43ef-93a1-6dc78f4c84ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2987\n",
            "126\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Specify the path to the video file\n",
        "video_path = \"/content/drive/MyDrive/model/720p.mp4\"\n",
        "\n",
        "# Specify the output folder to save the frames\n",
        "output_folder = \"/content/frames/720p/\"\n",
        "\n",
        "# Open the video file\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get the total number of frames in the video\n",
        "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "print(total_frames)\n",
        "\n",
        "# Calculate the frame interval to evenly sample the frames\n",
        "frame_interval = max(total_frames// 125 , 1)\n",
        "\n",
        "# Initialize a counter to keep track of the extracted frames\n",
        "frame_count_2 = 0\n",
        "\n",
        "# Loop through the frames and extract the desired number of frames\n",
        "while frame_count_2 <= 125 :\n",
        "    # Read the current frame\n",
        "    ret, frame = video.read()\n",
        "\n",
        "    # Check if the frame was successfully read\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Save the frame as an image file\n",
        "    frame_path = f\"{output_folder}{frame_count_2}.jpg\"\n",
        "    cv2.imwrite(frame_path, frame)\n",
        "\n",
        "    # Increment the frame count\n",
        "    frame_count_2 += 1\n",
        "\n",
        "    # Move to the next frame based on the frame interval\n",
        "    video.set(cv2.CAP_PROP_POS_FRAMES, frame_count_2 * frame_interval)\n",
        "\n",
        "# Release the video capture object\n",
        "video.release()\n",
        "print(frame_count_2)\n",
        "print(\"done.\")\n"
      ],
      "metadata": {
        "id": "71wOJrRx4VQD",
        "outputId": "0d66b07b-63cd-4b87-9bc3-1facc6ea401f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2986\n",
            "126\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "fnJ0iYIq4YZj",
        "outputId": "72ec2205-49db-4cd4-98d2-43c79133569b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import cv2\n",
        "import torchvision\n",
        "\n",
        "# Check if a GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the original 1080p image\n",
        "original_image = cv2.imread('/content/frames/360p/110.jpg')\n",
        "\n",
        "# Calculate the scale factor\n",
        "original_width = original_image.shape[1]\n",
        "target_width = 1920\n",
        "scale_factor = target_width / original_width\n",
        "\n",
        "# Downscale the image using OpenCV's resize function\n",
        "downscaled_image = cv2.resize(original_image, (int(original_width * scale_factor), int(original_image.shape[0] * scale_factor)))\n",
        "\n",
        "# Convert the images to tensors and move them to the GPU\n",
        "downscaled_image = transforms.ToTensor()(downscaled_image).unsqueeze(0).to(device)\n",
        "original_image = transforms.ToTensor()(original_image).unsqueeze(0).to(device)\n",
        "\n",
        "# Create a CNN model for upscaling\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.conv1(x))\n",
        "        out = self.conv2(out)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class UpscaleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UpscaleModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.residual_blocks = nn.Sequential(\n",
        "            ResidualBlock(64),\n",
        "            ResidualBlock(64),\n",
        "            ResidualBlock(64),\n",
        "            ResidualBlock(64),\n",
        "            ResidualBlock(64)\n",
        "        )\n",
        "        self.conv2 = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.residual_blocks(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.sigmoid(out + x)  # Add skip connection to preserve details\n",
        "        return out\n",
        "\n",
        "# Create the model and move it to the GPU\n",
        "model = UpscaleModel().to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Create a DataLoader for the training data\n",
        "train_dataset = TensorDataset(downscaled_image, original_image)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1)\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss = {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'upscale_model.pth')\n",
        "\n",
        "import gc\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "fbfzxkve_ZQ3",
        "outputId": "8844adde-57d2-45bd-a16f-258a1677edd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 0.1828\n",
            "Epoch 2: Loss = 0.1304\n",
            "Epoch 3: Loss = 0.0576\n",
            "Epoch 4: Loss = 0.0251\n",
            "Epoch 5: Loss = 0.0283\n",
            "Epoch 6: Loss = 0.0283\n",
            "Epoch 7: Loss = 0.0283\n",
            "Epoch 8: Loss = 0.0283\n",
            "Epoch 9: Loss = 0.0283\n",
            "Epoch 10: Loss = 0.0283\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import cv2\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "\n",
        "model = UpscaleModel()\n",
        "model.load_state_dict(torch.load('upscale_model.pth'))\n",
        "# Use the trained model to upscale the downscaled image\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    upscaled_image = model(downscaled_image)\n",
        "\n",
        "# Convert the upscaled image to a numpy array and move it to the CPU\n",
        "upscaled_image = upscaled_image.squeeze(0).cpu().numpy()\n",
        "\n",
        "# Upscale the image using OpenCV's resize function\n",
        "upscaled_image = cv2.resize(upscaled_image.transpose(1, 2, 0), (original_width, original_image.shape[2]))\n",
        "\n",
        "# Convert the upscaled image to a tensor and move it to the GPU\n",
        "upscaled_image = transforms.ToTensor()(upscaled_image).unsqueeze(0).to(device)\n",
        "\n",
        "# Calculate the SSIM and PSNR\n",
        "ssim_score = ssim(original_image.squeeze(0).permute(1, 2, 0).cpu().numpy(), upscaled_image.squeeze(0).permute(1, 2, 0).cpu().numpy(), multichannel=True)\n",
        "mse = torch.mean((original_image - upscaled_image)**2)\n",
        "psnr = 10 * torch.log10(1.0 / mse)\n",
        "\n",
        "print(f\"SSIM: {ssim_score:.4f}\")\n",
        "print(f\"PSNR: {psnr.item():.2f} dB\")"
      ],
      "metadata": {
        "id": "5ylHIO2O4dJ5",
        "outputId": "7fc61fc3-14d3-408b-f46a-d927797f45ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-b0d2cb2093b2>:27: FutureWarning: `multichannel` is a deprecated argument name for `structural_similarity`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
            "  ssim_score = ssim(original_image.squeeze(0).permute(1, 2, 0).cpu().numpy(), upscaled_image.squeeze(0).permute(1, 2, 0).cpu().numpy(), multichannel=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SSIM: 0.3818\n",
            "PSNR: 15.50 dB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import cv2\n",
        "\n",
        "# Check if a GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the original 720p image\n",
        "original_image = cv2.imread('/content/frames/360p/110.jpg')\n",
        "\n",
        "# Calculate the scale factor\n",
        "original_width = original_image.shape[1]\n",
        "target_width = 1920\n",
        "scale_factor = target_width / original_width\n",
        "\n",
        "# Downscale the image using OpenCV's resize function\n",
        "downscaled_image = cv2.resize(original_image, (int(original_width * scale_factor), int(original_image.shape[0] * scale_factor)))\n",
        "\n",
        "# Convert the images to tensors and move them to the GPU\n",
        "downscaled_image = transforms.ToTensor()(downscaled_image).unsqueeze(0).to(device)\n",
        "original_image = transforms.ToTensor()(original_image).unsqueeze(0).to(device)\n",
        "\n",
        "# Create a CNN model for upscaling\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.conv1(x))\n",
        "        out = self.conv2(out)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class UpscaleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UpscaleModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.residual_blocks = nn.Sequential(\n",
        "            ResidualBlock(64),\n",
        "            ResidualBlock(64),\n",
        "            ResidualBlock(64),\n",
        "            ResidualBlock(64),\n",
        "            ResidualBlock(64)\n",
        "        )\n",
        "        self.conv2 = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.residual_blocks(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.sigmoid(out + x)  # Add skip connection to preserve details\n",
        "        return out\n",
        "\n",
        "# Create the model and move it to the GPU\n",
        "model = UpscaleModel().to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Create a DataLoader for the training data\n",
        "train_dataset = TensorDataset(downscaled_image, original_image)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1)\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss = {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'upscale_model.pth')\n"
      ],
      "metadata": {
        "id": "nDqLOv7e4a9x"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}