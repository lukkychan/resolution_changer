{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukkychan/resolution_changer/blob/main/new3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnSzSCCZntxE",
        "outputId": "32234b04-f9ea-4a78-8233-65b27a15be35"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p frames/720p\n",
        "!mkdir -p frames/360p\n",
        "!mkdir scaled\n",
        "!mkdir predicted\n",
        "!mkdir dataset\n",
        "!mkdir -p dataset/720p\n",
        "!mkdir -p dataset/360p\n",
        "!mkdir checkpoints"
      ],
      "metadata": {
        "id": "KwoAhWwMnunq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Specify the path to the video file\n",
        "video_path = \"/content/drive/MyDrive/model/1080p.mp4\"\n",
        "\n",
        "# Specify the output folder to save the frames\n",
        "output_folder = \"/content/frames/360p/\"\n",
        "\n",
        "# Open the video file\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get the total number of frames in the video\n",
        "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "print(total_frames)\n",
        "\n",
        "# Calculate the frame interval to evenly sample the frames\n",
        "frame_interval = max(total_frames//125, 1)\n",
        "\n",
        "# Initialize a counter to keep track of the extracted frames\n",
        "frame_count_1 = 0\n",
        "\n",
        "# Loop through the frames and extract the desired number of frames\n",
        "while frame_count_1 <= 125 :\n",
        "    # Read the current frame\n",
        "    ret, frame = video.read()\n",
        "\n",
        "    # Check if the frame was successfully read\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Save the frame as an image file\n",
        "    frame_path = f\"{output_folder}{frame_count_1}.jpg\"\n",
        "    cv2.imwrite(frame_path, frame)\n",
        "\n",
        "    # Increment the frame count\n",
        "    frame_count_1 += 1\n",
        "\n",
        "    # Move to the next frame based on the frame interval\n",
        "    video.set(cv2.CAP_PROP_POS_FRAMES, frame_count_1 * frame_interval)\n",
        "\n",
        "# Release the video capture object\n",
        "video.release()\n",
        "print(frame_count_1)\n",
        "print(\"done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWVe1cHcn9Er",
        "outputId": "91d4fb56-4da3-4a7d-c2a7-bfb422865b28"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2987\n",
            "126\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Specify the path to the video file\n",
        "video_path = \"/content/drive/MyDrive/model/720p.mp4\"\n",
        "\n",
        "# Specify the output folder to save the frames\n",
        "output_folder = \"/content/frames/720p/\"\n",
        "\n",
        "# Open the video file\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get the total number of frames in the video\n",
        "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "print(total_frames)\n",
        "\n",
        "# Calculate the frame interval to evenly sample the frames\n",
        "frame_interval = max(total_frames// 125, 1)\n",
        "\n",
        "# Initialize a counter to keep track of the extracted frames\n",
        "frame_count_2 = 0\n",
        "\n",
        "# Loop through the frames and extract the desired number of frames\n",
        "while frame_count_2 <= 125 :\n",
        "    # Read the current frame\n",
        "    ret, frame = video.read()\n",
        "\n",
        "    # Check if the frame was successfully read\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Save the frame as an image file\n",
        "    frame_path = f\"{output_folder}{frame_count_2}.jpg\"\n",
        "    cv2.imwrite(frame_path, frame)\n",
        "\n",
        "    # Increment the frame count\n",
        "    frame_count_2 += 1\n",
        "\n",
        "    # Move to the next frame based on the frame interval\n",
        "    video.set(cv2.CAP_PROP_POS_FRAMES, frame_count_2 * frame_interval)\n",
        "\n",
        "# Release the video capture object\n",
        "video.release()\n",
        "print(frame_count_2)\n",
        "print(\"done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXYQ42oroJMK",
        "outputId": "fcae0d2b-0ba4-4da5-868a-ba4b8c72cab9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2986\n",
            "126\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import cv2\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "# Check if a GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the original 1080p image\n",
        "original_image = cv2.imread('/content/frames/360p/8.jpg')\n",
        "\n",
        "# Downscale the image to 720p using OpenCV's resize function\n",
        "downscaled_image = cv2.resize(original_image, (1280, 720))  # Assuming the original aspect ratio is 16:9\n",
        "\n",
        "# Convert the images to tensors and move them to the GPU\n",
        "downscaled_image = transforms.ToTensor()(downscaled_image).unsqueeze(0).to(device)\n",
        "original_image = transforms.ToTensor()(original_image).unsqueeze(0).to(device)\n",
        "\n",
        "# Create a CNN model for upscaling\n",
        "class UpscaleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UpscaleModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.up1 = nn.Upsample(scale_factor=1.875, mode='bilinear', align_corners=False)\n",
        "        self.conv2 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.up2 = nn.Upsample(scale_factor=1.5, mode='bilinear', align_corners=False)\n",
        "        self.conv3 = nn.Conv2d(32, 3, kernel_size=3, padding=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.up1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.up2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Create the model and move it to the GPU\n",
        "model = UpscaleModel().to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Create a DataLoader for the training data\n",
        "train_dataset = TensorDataset(downscaled_image, original_image)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1)\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss = {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Use the trained model to upscale the 720p image back to its original resolution\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    upscaled_image = model(downscaled_image).clamp(0, 1)\n",
        "\n",
        "# Convert the upscaled image to numpy array and move it to the CPU\n",
        "upscaled_image = upscaled_image.squeeze(0).cpu().numpy()\n",
        "\n",
        "# Upscale the image to its original resolution using OpenCV's resize function\n",
        "upscaled_image = cv2.resize(upscaled_image, (1920, 1080))\n",
        "\n",
        "# Convert the upscaled image to a tensor and move it to the GPU\n",
        "upscaled_image = transforms.ToTensor()(upscaled_image).unsqueeze(0).to(device)\n",
        "\n",
        "# Calculate SSIM\n",
        "ssim_score = ssim(original_image.squeeze(0).cpu().numpy().transpose(1, 2, 0), upscaled_image.squeeze(0).cpu().numpy().transpose(1, 2, 0), multichannel=True)\n",
        "\n",
        "# Calculate PSNR\n",
        "mse = torch.mean((original_image - upscaled_image)**2)\n",
        "psnr = 10 * torch.log10(255.0**2 / mse)\n",
        "\n",
        "print(f\"SSIM: {ssim_score:.4f}\")\n",
        "print(f\"PSNR: {psnr.item():.2f} dB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "YRW8nfxxoaFr",
        "outputId": "de2fe0b5-c527-46c0-aa59-caba61f2b939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 3, 1080, 1920])) that is different to the input size (torch.Size([1, 3, 2025, 3600])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-737778bd2223>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3292\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3294\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3295\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3600) must match the size of tensor b (1920) at non-singleton dimension 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dlKeTWVtHV7",
        "outputId": "6061ce31-0b84-4fbc-e69a-3c97c63c952a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import cv2\n",
        "\n",
        "# Check if a GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the original 1080p image\n",
        "original_image = cv2.imread('/content/frames/360p/8.jpg')\n",
        "\n",
        "# Downscale the image to 720p using OpenCV's resize function\n",
        "downscaled_image = cv2.resize(original_image, (1920, 1080))  # Assuming the original aspect ratio is 16:9\n",
        "\n",
        "# Convert the images to tensors and move them to the GPU\n",
        "downscaled_image = transforms.ToTensor()(downscaled_image).unsqueeze(0).to(device)\n",
        "original_image = transforms.ToTensor()(original_image).unsqueeze(0).to(device)\n",
        "\n",
        "# Create a CNN model for upscaling\n",
        "class UpscaleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UpscaleModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.up1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        self.conv2 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.up2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        self.conv3 = nn.Conv2d(32, 3, kernel_size=3, padding=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.up1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.up2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Create the model and move it to the GPU\n",
        "model = UpscaleModel().to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Create a DataLoader for the training data\n",
        "train_dataset = TensorDataset(downscaled_image, original_image)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1)\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss = {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Use the trained model to upscale the 720p image back to 1080p\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    upscaled_image = model(downscaled_image)\n",
        "\n",
        "# Convert the upscaled image tensor to a numpy array and remove the batch dimension\n",
        "upscaled_image = upscaled_image.squeeze(0).cpu().numpy()\n",
        "\n",
        "# Convert the image back to the range of 0-255 and change the channel order to BGR for OpenCV\n",
        "upscaled_image = (upscaled_image * 255).clip(0, 255).transpose(1, 2, 0).astype('uint8')\n",
        "\n",
        "# Save the upscaled image\n",
        "cv2.imwrite('/content/upscaled.jpg', upscaled_image)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "zSzY2J3XuEwp",
        "outputId": "0cb1c578-8558-4a42-8c54-0baf891abecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 3, 1080, 1920])) that is different to the input size (torch.Size([1, 3, 4320, 7680])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2d2f83468216>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3292\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3294\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3295\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (7680) must match the size of tensor b (1920) at non-singleton dimension 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import cv2\n",
        "import torchvision\n",
        "\n",
        "# Check if a GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the original 1080p image\n",
        "original_image = cv2.imread('/content/frames/360p/8.jpg')\n",
        "\n",
        "# Calculate the scale factor\n",
        "original_width = original_image.shape[1]\n",
        "target_width = 1920\n",
        "scale_factor = target_width / original_width\n",
        "\n",
        "# Downscale the image using OpenCV's resize function\n",
        "downscaled_image = cv2.resize(original_image, (int(original_width * scale_factor), int(original_image.shape[0] * scale_factor)))\n",
        "\n",
        "# Convert the images to tensors and move them to the GPU\n",
        "downscaled_image = transforms.ToTensor()(downscaled_image).unsqueeze(0).to(device)\n",
        "original_image = transforms.ToTensor()(original_image).unsqueeze(0).to(device)\n",
        "\n",
        "# Create a CNN model for upscaling\n",
        "class UpscaleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UpscaleModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.conv6 = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.relu(self.conv4(x))\n",
        "        x = self.relu(self.conv5(x))\n",
        "        x = self.conv6(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Create the model and move it to the GPU\n",
        "model = UpscaleModel().to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Create a DataLoader for the training data\n",
        "train_dataset = TensorDataset(downscaled_image, original_image)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1)\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss = {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'upscale_model.pth')\n",
        "\n",
        "import gc\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "Lqx2DLqAvA9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2nd"
      ],
      "metadata": {
        "id": "sT5iAoFMA9eA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import cv2\n",
        "import torchvision\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Check if a GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "image_paths = []\n",
        "for i in range(0,10):\n",
        "  image_paths.append(f\"/content/frames/360p/{i}.jpg\")\n",
        "\n",
        "\n",
        "# Create a CNN model for upscaling\n",
        "class UpscaleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UpscaleModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.conv6 = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.relu(self.conv4(x))\n",
        "        x = self.relu(self.conv5(x))\n",
        "        x = self.conv6(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "checkpoints = []\n",
        "\n",
        "for image_path in image_paths:\n",
        "  # Load the original 1080p image\n",
        "  original_image = cv2.imread(image_path)\n",
        "\n",
        "  # Calculate the scale factor\n",
        "  original_width = original_image.shape[1]\n",
        "  target_width = 1920\n",
        "  scale_factor = target_width / original_width\n",
        "\n",
        "  # Downscale the image using OpenCV's resize function\n",
        "  downscaled_image = cv2.resize(original_image, (int(original_width * scale_factor), int(original_image.shape[0] * scale_factor)))\n",
        "\n",
        "  # Convert the images to tensors and move them to the GPU\n",
        "  downscaled_image = transforms.ToTensor()(downscaled_image).unsqueeze(0).to(device)\n",
        "  original_image = transforms.ToTensor()(original_image).unsqueeze(0).to(device)\n",
        "  # Create the model and move it to the GPU\n",
        "  model = UpscaleModel().to(device)\n",
        "\n",
        "  # Define the loss function and optimizer\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  # Create a DataLoader for the training data\n",
        "  train_dataset = TensorDataset(downscaled_image, original_image)\n",
        "  train_loader = DataLoader(train_dataset, batch_size=2)\n",
        "\n",
        "  # Train the model\n",
        "  model.train()\n",
        "  best_loss = float('inf')  # Initialize best_loss\n",
        "  for epoch in range(10):\n",
        "      running_loss = 0.0\n",
        "      for inputs, labels in train_loader:\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "    # Check if the current loss is the best so far\n",
        "      if running_loss < best_loss:\n",
        "        best_loss = running_loss\n",
        "        # Save the current model checkpoint\n",
        "        checkpoints.append(model.state_dict())\n",
        "\n",
        "    # Check if the model is overfitting\n",
        "      if running_loss > 1.2 * best_loss:\n",
        "        print(\"Model is overfitting. Stopping training.\")\n",
        "        break\n",
        "      elif (running_loss/len(train_loader)) == 0.0000 :\n",
        "        break\n",
        "      else:\n",
        "        print(f\"Epoch {epoch+1}: Loss = {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'upscale_model.pth')\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(\"Execution time:\", execution_time/60)\n",
        "\n",
        "import gc\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6JbR2H-A9Ix",
        "outputId": "43554525-1f0e-4f17-a0ed-87117a35781f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 0.2474\n",
            "Epoch 2: Loss = 0.2330\n",
            "Epoch 3: Loss = 0.1757\n",
            "Epoch 4: Loss = 0.0599\n",
            "Epoch 5: Loss = 0.0010\n",
            "Epoch 6: Loss = 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import cv2\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "\n",
        "model = UpscaleModel()\n",
        "model.load_state_dict(torch.load('upscale_model.pth'))\n",
        "# Use the trained model to upscale the downscaled image\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    upscaled_image = model(downscaled_image)\n",
        "\n",
        "# Convert the upscaled image to a numpy array and move it to the CPU\n",
        "upscaled_image = upscaled_image.squeeze(0).cpu().numpy()\n",
        "\n",
        "# Upscale the image using OpenCV's resize function\n",
        "upscaled_image = cv2.resize(upscaled_image.transpose(1, 2, 0), (original_width, original_image.shape[2]))\n",
        "\n",
        "# Convert the upscaled image to a tensor and move it to the GPU\n",
        "upscaled_image = transforms.ToTensor()(upscaled_image).unsqueeze(0).to(device)\n",
        "\n",
        "# Calculate the SSIM and PSNR\n",
        "ssim_score = ssim(original_image.squeeze(0).permute(1, 2, 0).cpu().numpy(), upscaled_image.squeeze(0).permute(1, 2, 0).cpu().numpy(), multichannel=True)\n",
        "mse = torch.mean((original_image - upscaled_image)**2)\n",
        "psnr = 10 * torch.log10(1.0 / mse)\n",
        "\n",
        "print(f\"SSIM: {ssim_score:.4f}\")\n",
        "print(f\"PSNR: {psnr.item():.2f} dB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85HkZq_8xboy",
        "outputId": "b0aff327-7e48-41c6-9d64-5970be805a9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-b0d2cb2093b2>:27: FutureWarning: `multichannel` is a deprecated argument name for `structural_similarity`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
            "  ssim_score = ssim(original_image.squeeze(0).permute(1, 2, 0).cpu().numpy(), upscaled_image.squeeze(0).permute(1, 2, 0).cpu().numpy(), multichannel=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SSIM: 0.9713\n",
            "PSNR: 49.25 dB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "\n",
        "# Load the trained model\n",
        "model = UpscaleModel()\n",
        "model.load_state_dict(torch.load('upscale_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Load the downscaled image\n",
        "downscaled_image = cv2.imread('/content/frames/360p/110.jpg')\n",
        "\n",
        "# Convert the image to a tensor and normalize it\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "input_image = transform(downscaled_image).unsqueeze(0)\n",
        "\n",
        "# Upscale the image using the trained model\n",
        "with torch.no_grad():\n",
        "    upscaled_image = model(input_image)\n",
        "\n",
        "# Convert the upscaled image tensor to a numpy array\n",
        "upscaled_image = upscaled_image.squeeze(0).permute(1, 2, 0).numpy()\n",
        "\n",
        "# Convert the upscaled image back to the original range (0-255)\n",
        "upscaled_image = (upscaled_image * 255).astype('uint8')\n",
        "\n",
        "# Display or save the upscaled image\n",
        "#cv2.imshow('Upscaled Image', upscaled_image)\n",
        "cv2.imwrite(\"output.jpg\", upscaled_image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "Gdl9hbWj9_hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load the original and predicted images\n",
        "original_image = Image.open('/content/frames/360p/8.jpg')\n",
        "predicted_image = Image.open('/content/output.jpg')\n",
        "\n",
        "# Convert images to numpy arrays\n",
        "original_array = np.array(original_image)\n",
        "predicted_array = np.array(predicted_image.convert('RGB'))  # Convert predicted image to RGB mode\n",
        "\n",
        "# Calculate the pixel-wise difference\n",
        "diff_array = np.abs(original_array - predicted_array)\n",
        "\n",
        "# Calculate the number of mismatched pixels\n",
        "num_mismatched_pixels = np.sum(diff_array > 0)\n",
        "\n",
        "# Calculate the total number of pixels\n",
        "total_pixels = original_array.size\n",
        "\n",
        "# Calculate the accuracy as the percentage of matching pixels\n",
        "accuracy = ((total_pixels - num_mismatched_pixels) / total_pixels) * 100\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtwmCozB-qRA",
        "outputId": "807d27be-c390-4736-db39-d16ef6d6cd52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 22.73%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}