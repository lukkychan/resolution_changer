{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukkychan/resolution_changer/blob/main/today1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf frames\n",
        "!rm -rf frames/720p\n",
        "!rm -rf frames/360p\n",
        "!rm -rf scaled\n",
        "!rm -rf predicted\n",
        "!rm -rf Pre_orginal\n",
        "!rm -rf dataset\n",
        "!rm -rf dataset/720p\n",
        "!rm -rf dataset/360s\n",
        "!rm -rf checkpoints\n",
        "!rm -rf temp_model\n",
        "!rm -rf temp_scaled"
      ],
      "metadata": {
        "id": "dRzMWFsPblZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tapjV_tTS0mC",
        "outputId": "1a7c2cb2-b61f-4a85-898a-7580685f1563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p frames/720p\n",
        "!mkdir -p frames/360p\n",
        "!mkdir scaled\n",
        "!mkdir predicted\n",
        "!mkdir dataset\n",
        "!mkdir -p dataset/720p\n",
        "!mkdir -p dataset/360s\n",
        "!mkdir checkpoints"
      ],
      "metadata": {
        "id": "L-gSuwr_Seqg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Specify the path to the video file\n",
        "video_path = \"/content/drive/MyDrive/model/360p.mp4\"\n",
        "\n",
        "# Specify the output folder to save the frames\n",
        "output_folder = \"/content/frames/360p/\"\n",
        "\n",
        "# Open the video file\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get the total number of frames in the video\n",
        "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "print(total_frames)\n",
        "\n",
        "# Calculate the frame interval to evenly sample the frames\n",
        "frame_interval = max(total_frames // 150, 1)\n",
        "\n",
        "# Initialize a counter to keep track of the extracted frames\n",
        "frame_count = 0\n",
        "\n",
        "# Loop through the frames and extract the desired number of frames\n",
        "while frame_count <= 150:\n",
        "    # Read the current frame\n",
        "    ret, frame = video.read()\n",
        "\n",
        "    # Check if the frame was successfully read\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Save the frame as an image file\n",
        "    frame_path = f\"{output_folder}{frame_count:03d}.jpg\"\n",
        "    cv2.imwrite(frame_path, frame)\n",
        "\n",
        "    # Increment the frame count\n",
        "    frame_count += 1\n",
        "\n",
        "    # Move to the next frame based on the frame interval\n",
        "    video.set(cv2.CAP_PROP_POS_FRAMES, frame_count * frame_interval)\n",
        "\n",
        "# Release the video capture object\n",
        "video.release()\n",
        "print(\"done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfYfWuJiTVgv",
        "outputId": "7340cda8-79d4-453b-ece3-9ddab7e2f764"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2987\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Specify the path to the video file\n",
        "video_path = \"/content/drive/MyDrive/model/720p.mp4\"\n",
        "\n",
        "# Specify the output folder to save the frames\n",
        "output_folder = \"/content/frames/720p/\"\n",
        "\n",
        "# Open the video file\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get the total number of frames in the video\n",
        "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "print(total_frames)\n",
        "\n",
        "# Calculate the frame interval to evenly sample the frames\n",
        "frame_interval = max(total_frames // 150, 1)\n",
        "\n",
        "# Initialize a counter to keep track of the extracted frames\n",
        "frame_count = 0\n",
        "\n",
        "# Loop through the frames and extract the desired number of frames\n",
        "while frame_count <= 150:\n",
        "    # Read the current frame\n",
        "    ret, frame = video.read()\n",
        "\n",
        "    # Check if the frame was successfully read\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Save the frame as an image file\n",
        "    frame_path = f\"{output_folder}{frame_count:03d}.jpg\"\n",
        "    cv2.imwrite(frame_path, frame)\n",
        "\n",
        "    # Increment the frame count\n",
        "    frame_count += 1\n",
        "\n",
        "    # Move to the next frame based on the frame interval\n",
        "    video.set(cv2.CAP_PROP_POS_FRAMES, frame_count * frame_interval)\n",
        "\n",
        "# Release the video capture object\n",
        "video.release()\n",
        "print(\"done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbt0L-FfTbyQ",
        "outputId": "a62e3935-2e71-40de-da19-8096335e3932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2986\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "def scale_images(input_folder, output_folder):\n",
        "    # Create the output folder if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Get a list of all image files in the input folder\n",
        "    image_files = [f for f in os.listdir(input_folder) if os.path.isfile(os.path.join(input_folder, f))]\n",
        "\n",
        "    for image_file in image_files:\n",
        "        # Construct the input and output file paths\n",
        "        input_path = os.path.join(input_folder, image_file)\n",
        "        output_path = os.path.join(output_folder, image_file)\n",
        "\n",
        "        # Execute FFmpeg command to scale the image to 1920x1080\n",
        "        command = ['ffmpeg', '-i', input_path, '-vf', 'scale=1280:720', output_path]\n",
        "        subprocess.call(command)\n",
        "\n",
        "# Specify the paths to the input folder and output folder\n",
        "input_folder = '/content/frames/360p/'\n",
        "output_folder = '/content/scaled/'\n",
        "\n",
        "# Call the function to scale the images\n",
        "scale_images(input_folder, output_folder)\n",
        "print(\"scaling done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z30dMjXoThTA",
        "outputId": "b87ce8f8-7128-45ae-fc3f-32bce0978310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scaling done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Specify the directory containing the original 1080p images\n",
        "original_dir = '/content/frames/720p/'\n",
        "\n",
        "# Create a directory to save the cropped images\n",
        "#os.makedirs('path/to/save/cropped/images', exist_ok=True)\n",
        "counter = 1\n",
        "# Iterate over the images in the original directory\n",
        "for filename in os.listdir(original_dir):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):  # Adjust file extensions if needed\n",
        "        # Load the original 1080p image\n",
        "        original_path = os.path.join(original_dir, filename)\n",
        "        original_image = cv2.imread(original_path)\n",
        "\n",
        "        # Calculate the dimensions of each cropped part\n",
        "        image_height, image_width, _ = original_image.shape\n",
        "        crop_height = image_height // 2\n",
        "        crop_width = image_width // 2\n",
        "\n",
        "        # Crop the image into 9 equal parts and save them in ascending order\n",
        "\n",
        "        for i in range(2):\n",
        "            for j in range(2):\n",
        "                start_x = j * crop_width\n",
        "                start_y = i * crop_height\n",
        "                end_x = start_x + crop_width\n",
        "                end_y = start_y + crop_height\n",
        "\n",
        "                cropped_image = original_image[start_y:end_y, start_x:end_x]\n",
        "\n",
        "                # Save the cropped image\n",
        "                save_path = os.path.join('/content/dataset/720p/', f'{counter:03d}.jpg')\n",
        "                cv2.imwrite(save_path, cropped_image)\n",
        "\n",
        "                counter += 1\n",
        "\n",
        "\n",
        "# Print the size of the cropped image\n",
        "cropped_height, cropped_width, _ = cropped_image.shape\n",
        "print(f\"Cropped Image {counter} ({filename}) size: {cropped_width}x{cropped_height}\")\n",
        "print(counter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfsJYMPBT-qn",
        "outputId": "faceceef-b3b7-41ad-e602-aa87452a600e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cropped Image 405 (038.jpg) size: 640x360\n",
            "405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Specify the directory containing the original 1080p images\n",
        "original_dir = '/content/scaled/'\n",
        "\n",
        "# Create a directory to save the cropped images\n",
        "#os.makedirs('path/to/save/cropped/images', exist_ok=True)\n",
        "counter = 1\n",
        "# Iterate over the images in the original directory\n",
        "for filename in os.listdir(original_dir):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):  # Adjust file extensions if needed\n",
        "        # Load the original 1080p image\n",
        "        original_path = os.path.join(original_dir, filename)\n",
        "        original_image = cv2.imread(original_path)\n",
        "\n",
        "        # Calculate the dimensions of each cropped part\n",
        "        image_height, image_width, _ = original_image.shape\n",
        "        crop_height = image_height // 2\n",
        "        crop_width = image_width // 2\n",
        "\n",
        "        # Crop the image into 9 equal parts and save them in ascending order\n",
        "\n",
        "        for i in range(2):\n",
        "            for j in range(2):\n",
        "                start_x = j * crop_width\n",
        "                start_y = i * crop_height\n",
        "                end_x = start_x + crop_width\n",
        "                end_y = start_y + crop_height\n",
        "\n",
        "                cropped_image = original_image[start_y:end_y, start_x:end_x]\n",
        "\n",
        "                # Save the cropped image\n",
        "                save_path = os.path.join('/content/dataset/360s/', f'{counter:03d}.jpg')\n",
        "                cv2.imwrite(save_path, cropped_image)\n",
        "\n",
        "                counter += 1\n",
        "\n",
        "\n",
        "# Print the size of the cropped image\n",
        "cropped_height, cropped_width, _ = cropped_image.shape\n",
        "print(f\"Cropped Image {counter} ({filename}) size: {cropped_width}x{cropped_height}\")\n",
        "print(counter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6vC-OVcUXDA",
        "outputId": "240e99e0-f99f-4417-c9b9-caa5305c52f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cropped Image 405 (038.jpg) size: 640x360\n",
            "405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "gc.collect()\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "a39vVRNceD6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.transforms import ToPILImage\n",
        "\n",
        "\n",
        "# Step 1: Organize data and directories\n",
        "original_dir = '/content/dataset/720p/'  # Directory containing original 1080p images\n",
        "scaled_dir = '/content/dataset/360s/'  # Directory containing scaled images\n",
        "\n",
        "# Step 2: Load and preprocess the images\n",
        "original_images = []\n",
        "scaled_images = []\n",
        "\n",
        "for filename in os.listdir(original_dir):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):  # Adjust file extensions if needed\n",
        "        original_path = os.path.join(original_dir, filename)\n",
        "        scaled_path = os.path.join(scaled_dir, filename)\n",
        "\n",
        "        original_image = cv2.imread(original_path)\n",
        "        scaled_image = cv2.imread(scaled_path)\n",
        "\n",
        "        # Preprocess the images (resize and normalize)\n",
        "        original_image = cv2.resize(original_image, (640, 360))\n",
        "        scaled_image = cv2.resize(scaled_image, (640, 360))\n",
        "\n",
        "        original_images.append(original_image)\n",
        "        scaled_images.append(scaled_image)\n",
        "\n",
        "original_images = np.array(original_images)\n",
        "scaled_images = np.array(scaled_images)\n",
        "\n",
        "original_images = original_images.transpose((0, 3, 1, 2))  # Transpose from (num_samples, 640, 360, 3) to (num_samples, 3, 640, 360)\n",
        "scaled_images = scaled_images.transpose((0, 3, 1, 2))  # Transpose from (num_samples, 640, 360, 3) to (num_samples, 3, 640, 360)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Convert the input data to the correct shape and move to GPU\n",
        "original_images = torch.from_numpy(original_images).to(device).float()\n",
        "scaled_images = torch.from_numpy(scaled_images).to(device).float()\n",
        "\n",
        "# Define the U-Net model architecture\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.encoder2 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.encoder3 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.encoder4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder4 = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        )\n",
        "        self.decoder3 = nn.Sequential(\n",
        "            nn.Conv2d(512, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        )\n",
        "        self.decoder2 = nn.Sequential(\n",
        "            nn.Conv2d(256, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        )\n",
        "        self.decoder1 = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 3, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = self.encoder1(x)\n",
        "        x2 = self.encoder2(x1)\n",
        "        x3 = self.encoder3(x2)\n",
        "        x4 = self.encoder4(x3)\n",
        "\n",
        "        # Decoder\n",
        "        d4 = self.decoder4(x4)\n",
        "        d3 = self.decoder3(torch.cat([d4, x3], dim=1))\n",
        "        d2 = self.decoder2(torch.cat([d3, x2], dim=1))\n",
        "        d1 = self.decoder1(torch.cat([d2, x1], dim=1))\n",
        "\n",
        "        return d1\n",
        "\n",
        "\n",
        "# Create an instance of the U-Net model\n",
        "model = UNet()\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Print the model architecture\n",
        "#print(model)\n",
        "\n",
        "# Step 5: Compile and train the model\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Define a custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, inputs, targets, transform=None):\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_image = self.inputs[idx]\n",
        "        target_image = self.targets[idx]\n",
        "\n",
        "        # Convert tensors to PIL images\n",
        "        input_image = ToPILImage()(input_image)\n",
        "        target_image = ToPILImage()(target_image)\n",
        "\n",
        "        if self.transform:\n",
        "            input_image = self.transform(input_image)\n",
        "            target_image = self.transform(target_image)\n",
        "\n",
        "        return input_image, target_image\n",
        "\n",
        "\n",
        "# Create dataloaders for training and validation\n",
        "train_dataset = CustomDataset(scaled_images, original_images, transform=ToTensor())\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_dataloader:\n",
        "        inputs = inputs.to(device).float()  # Convert input data to float\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {running_loss}\")\n",
        "\n",
        "# Step 6: Save the trained model\n",
        "torch.save(model.state_dict(), '/content/model.pth')\n",
        "print(\"Model saved.\")\n",
        "\n",
        "# Perform memory cleanup\n",
        "gc.collect()\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Q8SXx3xCU6Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Organize data and directories\n",
        "original_dir = '/content/dataset/720p/'  # Directory containing original 1080p images\n",
        "scaled_dir = '/content/dataset/360s/'  # Directory containing scaled images\n",
        "\n",
        "# Step 2: Load and preprocess the images\n",
        "original_images = []\n",
        "scaled_images = []\n",
        "\n",
        "for filename in os.listdir(original_dir):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):  # Adjust file extensions if needed\n",
        "        original_path = os.path.join(original_dir, filename)\n",
        "        scaled_path = os.path.join(scaled_dir, filename)\n",
        "\n",
        "        original_image = cv2.imread(original_path)\n",
        "        scaled_image = cv2.imread(scaled_path)\n",
        "\n",
        "        # Preprocess the images (resize and normalize)\n",
        "        original_image = cv2.resize(original_image, (640, 360))\n",
        "        scaled_image = cv2.resize(scaled_image, (640, 360))\n",
        "\n",
        "        original_images.append(original_image)\n",
        "        scaled_images.append(scaled_image)\n",
        "\n",
        "original_images = np.array(original_images)\n",
        "scaled_images = np.array(scaled_images)\n",
        "\n",
        "original_images = original_images.transpose((0, 3, 1, 2))  # Transpose from (num_samples, 640, 360, 3) to (num_samples, 3, 640, 360)\n",
        "scaled_images = scaled_images.transpose((0, 3, 1, 2))  # Transpose from (num_samples, 640, 360, 3) to (num_samples, 3, 640, 360)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Convert the input data to the correct shape and move to GPU\n",
        "original_images = torch.from_numpy(original_images).to(device).float()\n",
        "scaled_images = torch.from_numpy(scaled_images).to(device).float()\n",
        "\n",
        "# Define the U-Net model architecture\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.encoder2 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.encoder3 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.encoder4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder5 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        )\n",
        "        self.decoder6 = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        )\n",
        "        self.decoder7 = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        )\n",
        "        self.decoder8 = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 3, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = self.encoder1(x)\n",
        "        x2 = self.encoder2(x1)\n",
        "        x3 = self.encoder3(x2)\n",
        "        x4 = self.encoder4(x3)\n",
        "\n",
        "        # Decoder\n",
        "        d5 = self.decoder5(x4)\n",
        "        d4 = self.decoder6(torch.cat([d5, x3], dim=1))\n",
        "        d3 = self.decoder7(torch.cat([d4, x2], dim=1))\n",
        "        d2 = self.decoder8(torch.cat([d3, x1], dim=1))\n",
        "\n",
        "        return d2\n",
        "\n",
        "# Step 2: Load the previous model checkpoint\n",
        "#checkpoint_path = '/content/checkpoints/model_epoch_6.pth'  # Path to the previous model checkpoint file\n",
        "\n",
        "# Create an instance of the U-Net model\n",
        "model = UNet()\n",
        "\n",
        "# Load the model checkpoint\n",
        "# checkpoint = torch.load(checkpoint_path)\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Move the model to GPU if available\n",
        "model.to(device)\n",
        "\n",
        "# Print the model architecture\n",
        "#print(model)\n",
        "\n",
        "# Step 5: Compile and train the model\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, verbose=True)\n",
        "# Create lists to store training and validation losses\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Define a custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, inputs, targets, transform=None):\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_image = self.inputs[idx]\n",
        "        target_image = self.targets[idx]\n",
        "\n",
        "        # Convert tensors to PIL images\n",
        "        input_image = ToPILImage()(input_image)\n",
        "        target_image = ToPILImage()(target_image)\n",
        "\n",
        "        if self.transform:\n",
        "            input_image = self.transform(input_image)\n",
        "            target_image = self.transform(target_image)\n",
        "\n",
        "        return input_image, target_image\n",
        "\n",
        "\n",
        "# Create dataloaders for training and validation\n",
        "train_dataset = CustomDataset(scaled_images, original_images, transform=ToTensor())\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "val_dataset = CustomDataset(scaled_images, original_images, transform=ToTensor())  # Replace with your validation dataset\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(7):\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_dataloader:\n",
        "        inputs = inputs.to(device).float()  # Convert input data to float\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {running_loss}\")\n",
        "\n",
        "    # Calculate training loss\n",
        "    train_loss = running_loss / len(train_dataloader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Calculate validation loss\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        for val_inputs, val_targets in val_dataloader:\n",
        "            val_inputs = val_inputs.to(device).float()\n",
        "            val_targets = val_targets.to(device)\n",
        "\n",
        "            val_outputs = model(val_inputs)\n",
        "            loss = criterion(val_outputs, val_targets)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "        val_loss /= len(val_dataloader)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "    # Adjust learning rate based on the validation loss\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Check for overfitting\n",
        "    if epoch > 0:\n",
        "        if val_losses[epoch] > val_losses[epoch-1]:\n",
        "            print(\"Model is starting to overfit. Stopping training.\")\n",
        "            break\n",
        "\n",
        "    # Save checkpoint after every epoch\n",
        "    checkpoint_path = f'/content/checkpoints/model_epoch_{epoch+1}.pth'\n",
        "    torch.save({\n",
        "        'epoch': epoch+1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': running_loss\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at: {checkpoint_path}\")\n",
        "\n",
        "# Plot training and validation losses\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Step 6: Save the trained model\n",
        "torch.save(model.state_dict(), '/content/model.pth')\n",
        "print(\"Model saved.\")\n",
        "\n",
        "\n",
        "# Perform memory cleanup\n",
        "gc.collect()\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "ahGxAQN1lrUR",
        "outputId": "0890ff06-a1ed-47c1-86a5-f40975a8d1f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 5.1346632316708565\n",
            "Checkpoint saved at: /content/checkpoints/model_epoch_1.pth\n",
            "Epoch: 2, Loss: 1.4297651257365942\n",
            "Checkpoint saved at: /content/checkpoints/model_epoch_2.pth\n",
            "Epoch: 3, Loss: 1.2971472945064306\n",
            "Checkpoint saved at: /content/checkpoints/model_epoch_3.pth\n",
            "Epoch: 4, Loss: 1.198426142334938\n",
            "Checkpoint saved at: /content/checkpoints/model_epoch_4.pth\n",
            "Epoch: 5, Loss: 1.135023782029748\n",
            "Model is starting to overfit. Stopping training.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "\n",
        "# Load the trained model\n",
        "model = UNet()\n",
        "model.load_state_dict(torch.load('/content/model.pth'))\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Load and preprocess the input image\n",
        "input_image = cv2.imread('/content/dataset/360s/045.jpg')\n",
        "input_image = cv2.resize(input_image, (640, 360))\n",
        "input_tensor = ToTensor()(input_image).unsqueeze(0).to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    # Forward pass\n",
        "    output_tensor = model(input_tensor)\n",
        "\n",
        "# Convert the output tensor to a numpy array\n",
        "output_image = output_tensor.squeeze(0).cpu().numpy()\n",
        "\n",
        "# Convert the output tensor to a numpy array\n",
        "output_image = output_tensor.squeeze(0).cpu().numpy()\n",
        "\n",
        "# Transpose the output array to match the shape (height, width, channels)\n",
        "output_image = np.transpose(output_image, (1, 2, 0))\n",
        "\n",
        "# Scale the pixel values from [0, 1] to [0, 255]\n",
        "output_image = (output_image * 255).astype(np.uint8)\n",
        "\n",
        "# Convert the output array to a PIL image\n",
        "output_image = Image.fromarray(output_image)\n",
        "\n",
        "# Save the predicted image\n",
        "output_image.save('/content/predicted/predicted_image.jpg')"
      ],
      "metadata": {
        "id": "wgp9HbU2VCHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load the original and predicted images\n",
        "original_image = Image.open('/content/dataset/720p/045.jpg')\n",
        "predicted_image = Image.open('/content/predicted/predicted_image.jpg')\n",
        "\n",
        "# Convert images to numpy arrays\n",
        "original_array = np.array(original_image)\n",
        "predicted_array = np.array(predicted_image)\n",
        "\n",
        "# Calculate the pixel-wise difference\n",
        "diff_array = np.abs(original_array - predicted_array)\n",
        "\n",
        "# Calculate the number of mismatched pixels\n",
        "num_mismatched_pixels = np.sum(diff_array > 0)\n",
        "\n",
        "# Calculate the total number of pixels\n",
        "total_pixels = original_array.size\n",
        "\n",
        "# Calculate the accuracy as the percentage of matching pixels\n",
        "accuracy = ((total_pixels - num_mismatched_pixels) / total_pixels) * 100\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvlEYVv9VFMP",
        "outputId": "dc493014-61a9-422b-858d-7d808c5f30b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 3.07%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf temp_scaled\n",
        "!mkdir temp_scaled\n",
        "!cp /content/checkpoints/model_epoch_6.pth /content/temp_model/model_epoch_6.pth"
      ],
      "metadata": {
        "id": "0x2xymyb1Q5r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}