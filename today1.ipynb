{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukkychan/resolution_changer/blob/main/today1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf frames\n",
        "!rm -rf frames/720p\n",
        "!rm -rf frames/360p\n",
        "!rm -rf scaled\n",
        "!rm -rf predicted\n",
        "!rm -rf Pre_orginal\n",
        "!rm -rf dataset\n",
        "!rm -rf dataset/720p\n",
        "!rm -rf dataset/360s\n",
        "!rm -rf checkpoints\n",
        "!rm -rf temp_model\n",
        "!rm -rf temp_scaled"
      ],
      "metadata": {
        "id": "dRzMWFsPblZt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tapjV_tTS0mC",
        "outputId": "b912a3e8-b9ac-4b89-ad76-d4b7d6645df1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p frames/720p\n",
        "!mkdir -p frames/360p\n",
        "!mkdir scaled\n",
        "!mkdir predicted\n",
        "!mkdir dataset\n",
        "!mkdir -p dataset/720p\n",
        "!mkdir -p dataset/360p\n",
        "!mkdir checkpoints"
      ],
      "metadata": {
        "id": "L-gSuwr_Seqg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Specify the path to the video file\n",
        "video_path = \"/content/drive/MyDrive/model/360p.mp4\"\n",
        "\n",
        "# Specify the output folder to save the frames\n",
        "output_folder = \"/content/frames/360p/\"\n",
        "\n",
        "# Open the video file\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get the total number of frames in the video\n",
        "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "print(total_frames)\n",
        "\n",
        "# Calculate the frame interval to evenly sample the frames\n",
        "frame_interval = max(total_frames//total_frames, 1)\n",
        "\n",
        "# Initialize a counter to keep track of the extracted frames\n",
        "frame_count_1 = 0\n",
        "\n",
        "# Loop through the frames and extract the desired number of frames\n",
        "while True:\n",
        "    # Read the current frame\n",
        "    ret, frame = video.read()\n",
        "\n",
        "    # Check if the frame was successfully read\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Save the frame as an image file\n",
        "    frame_path = f\"{output_folder}{frame_count_1}.jpg\"\n",
        "    cv2.imwrite(frame_path, frame)\n",
        "\n",
        "    # Increment the frame count\n",
        "    frame_count_1 += 1\n",
        "\n",
        "    # Move to the next frame based on the frame interval\n",
        "    video.set(cv2.CAP_PROP_POS_FRAMES, frame_count_1 * frame_interval)\n",
        "\n",
        "# Release the video capture object\n",
        "video.release()\n",
        "print(frame_count_1)\n",
        "print(\"done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfYfWuJiTVgv",
        "outputId": "bbd8155f-6609-452b-8f6c-4d9bde5acb69"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2987\n",
            "2987\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Specify the path to the video file\n",
        "video_path = \"/content/drive/MyDrive/model/720p.mp4\"\n",
        "\n",
        "# Specify the output folder to save the frames\n",
        "output_folder = \"/content/frames/720p/\"\n",
        "\n",
        "# Open the video file\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get the total number of frames in the video\n",
        "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "print(total_frames)\n",
        "\n",
        "# Calculate the frame interval to evenly sample the frames\n",
        "frame_interval = max(total_frames // total_frames, 1)\n",
        "\n",
        "# Initialize a counter to keep track of the extracted frames\n",
        "frame_count_2 = 0\n",
        "\n",
        "# Loop through the frames and extract the desired number of frames\n",
        "while True:\n",
        "    # Read the current frame\n",
        "    ret, frame = video.read()\n",
        "\n",
        "    # Check if the frame was successfully read\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Save the frame as an image file\n",
        "    frame_path = f\"{output_folder}{frame_count_2}.jpg\"\n",
        "    cv2.imwrite(frame_path, frame)\n",
        "\n",
        "    # Increment the frame count\n",
        "    frame_count_2 += 1\n",
        "\n",
        "    # Move to the next frame based on the frame interval\n",
        "    video.set(cv2.CAP_PROP_POS_FRAMES, frame_count_2 * frame_interval)\n",
        "\n",
        "# Release the video capture object\n",
        "video.release()\n",
        "print(frame_count_2)\n",
        "print(\"done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbt0L-FfTbyQ",
        "outputId": "622f597c-f67c-42a9-daa5-a8c5e60585cb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2986\n",
            "2986\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "output_folder_1 = \"/content/frames/360p/\"\n",
        "output_folder_2 = \"/content/frames/720p/\"\n",
        "while True:\n",
        "  if frame_count_1 == frame_count_2:\n",
        "    break\n",
        "  elif frame_count_1 > frame_count_2:\n",
        "    os.remove(f\"{output_folder_1}{frame_count_1-1}.jpg\")\n",
        "    frame_count_1-=1\n",
        "  else:\n",
        "    os.remove(f\"{output_folder_2}{frame_count_2-1}.jpg\")\n",
        "    frame_count_2-=1\n",
        "print(frame_count_1,frame_count_2)"
      ],
      "metadata": {
        "id": "jmKLmsYv-ayP",
        "outputId": "c8904a93-cc06-4236-d0ad-3cc3644e88ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2986 2986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "def scale_images(input_folder, output_folder):\n",
        "    # Create the output folder if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Get a list of all image files in the input folder\n",
        "    image_files = [f for f in os.listdir(input_folder) if os.path.isfile(os.path.join(input_folder, f))]\n",
        "\n",
        "    for image_file in image_files:\n",
        "        # Construct the input and output file paths\n",
        "        input_path = os.path.join(input_folder, image_file)\n",
        "        output_path = os.path.join(output_folder, image_file)\n",
        "\n",
        "        # Execute FFmpeg command to scale the image to 1920x1080\n",
        "        command = ['ffmpeg', '-i', input_path, '-vf', 'scale=1280:720', output_path]\n",
        "        subprocess.call(command)\n",
        "\n",
        "# Specify the paths to the input folder and output folder\n",
        "input_folder = '/content/frames/360p/'\n",
        "output_folder = '/content/scaled/'\n",
        "\n",
        "# Call the function to scale the images\n",
        "scale_images(input_folder, output_folder)\n",
        "print(\"scaling done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z30dMjXoThTA",
        "outputId": "60e17b47-b49b-4e7e-e7ad-b020b79687a9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scaling done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Specify the directory containing the original 1080p images\n",
        "original_dir = '/content/scaled/'\n",
        "\n",
        "# Create a directory to save the cropped images\n",
        "#os.makedirs('path/to/save/cropped/images', exist_ok=True)\n",
        "counter1 = 0\n",
        "# Iterate over the images in the original directory\n",
        "for filename in os.listdir(original_dir):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):  # Adjust file extensions if needed\n",
        "        # Load the original 1080p image\n",
        "        original_path = os.path.join(original_dir, filename)\n",
        "        original_image = cv2.imread(original_path)\n",
        "\n",
        "        # Calculate the dimensions of each cropped part\n",
        "        image_height, image_width, _ = original_image.shape\n",
        "        crop_height = image_height // 2\n",
        "        crop_width = image_width // 2\n",
        "\n",
        "        # Crop the image into 9 equal parts and save them in ascending order\n",
        "\n",
        "        for i in range(2):\n",
        "            for j in range(2):\n",
        "                start_x = j * crop_width\n",
        "                start_y = i * crop_height\n",
        "                end_x = start_x + crop_width\n",
        "                end_y = start_y + crop_height\n",
        "\n",
        "                cropped_image = original_image[start_y:end_y, start_x:end_x]\n",
        "\n",
        "                # Save the cropped image\n",
        "                save_path = os.path.join('/content/dataset/360p/', f'{counter1}.jpg')\n",
        "                cv2.imwrite(save_path, cropped_image)\n",
        "\n",
        "                counter1 += 1\n",
        "\n",
        "\n",
        "# Print the size of the cropped image\n",
        "cropped_height, cropped_width, _ = cropped_image.shape\n",
        "print(f\"Cropped Image {counter1} ({filename}) size: {cropped_width}x{cropped_height}\")\n",
        "print(counter1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6vC-OVcUXDA",
        "outputId": "9efc30de-496d-4edc-d331-259b9e24f17c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cropped Image 11945 (1080.jpg) size: 640x360\n",
            "11944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "source_folder = \"/content/dataset/360p/\"\n",
        "destination_folder_base = \"/content/dataset/360p/\"\n",
        "num_images_per_folder = 125\n",
        "\n",
        "image_count = 0\n",
        "folder_count = 0\n",
        "\n",
        "total_folders = counter1 // 125\n",
        "print(total_folders)\n",
        "\n",
        "for i in range(total_folders):\n",
        "    # Create the destination folder path\n",
        "    destination_folder = f\"{destination_folder_base}{i + 1}\"\n",
        "\n",
        "    # Check if the destination folder already exists\n",
        "    if not os.path.exists(destination_folder):\n",
        "        os.makedirs(destination_folder)\n",
        "\n",
        "    # Move images to the destination folder\n",
        "    for j in range(image_count + 1, image_count + 126):\n",
        "        image = f\"{j}.jpg\"\n",
        "        source_path = os.path.join(source_folder, image)\n",
        "        destination_path = os.path.join(destination_folder, image)\n",
        "\n",
        "        # Check if the source image file exists\n",
        "        if os.path.exists(source_path):\n",
        "            shutil.move(source_path, destination_path)\n",
        "\n",
        "    # Update the counters\n",
        "    image_count += num_images_per_folder\n",
        "\n",
        "print(\"All images have been moved.\")\n",
        "remaining_images = [f for f in os.listdir(source_folder) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "print(len(remaining_images))\n"
      ],
      "metadata": {
        "id": "BkQn98_G5gQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Specify the directory containing the original 1080p images\n",
        "original_dir = '/content/frames/720p/'\n",
        "\n",
        "# Create a directory to save the cropped images\n",
        "#os.makedirs('path/to/save/cropped/images', exist_ok=True)\n",
        "counter2 = 0\n",
        "# Iterate over the images in the original directory\n",
        "for filename in os.listdir(original_dir):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):  # Adjust file extensions if needed\n",
        "        # Load the original 1080p image\n",
        "        original_path = os.path.join(original_dir, filename)\n",
        "        original_image = cv2.imread(original_path)\n",
        "\n",
        "        # Calculate the dimensions of each cropped part\n",
        "        image_height, image_width, _ = original_image.shape\n",
        "        crop_height = image_height // 2\n",
        "        crop_width = image_width // 2\n",
        "\n",
        "        # Crop the image into 9 equal parts and save them in ascending order\n",
        "\n",
        "        for i in range(2):\n",
        "            for j in range(2):\n",
        "                start_x = j * crop_width\n",
        "                start_y = i * crop_height\n",
        "                end_x = start_x + crop_width\n",
        "                end_y = start_y + crop_height\n",
        "\n",
        "                cropped_image = original_image[start_y:end_y, start_x:end_x]\n",
        "\n",
        "                # Save the cropped image\n",
        "                save_path = os.path.join('/content/dataset/720p/', f'{counter2}.jpg')\n",
        "                cv2.imwrite(save_path, cropped_image)\n",
        "\n",
        "                counter2 += 1\n",
        "\n",
        "\n",
        "# Print the size of the cropped image\n",
        "cropped_height, cropped_width, _ = cropped_image.shape\n",
        "print(f\"Cropped Image {counter2} ({filename}) size: {cropped_width}x{cropped_height}\")\n",
        "print(counter2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfsJYMPBT-qn",
        "outputId": "644e173e-721c-4bae-8f64-acfd19646ad7"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cropped Image 11944 (1080.jpg) size: 640x360\n",
            "11944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "source_folder = \"/content/dataset/720p/\"\n",
        "destination_folder_base = \"/content/dataset/720p/\"\n",
        "num_images_per_folder = 125\n",
        "\n",
        "image_count = 0\n",
        "folder_count = 0\n",
        "\n",
        "total_folders = counter1 // 125\n",
        "print(total_folders)\n",
        "\n",
        "for i in range(total_folders):\n",
        "    # Create the destination folder path\n",
        "    destination_folder = f\"{destination_folder_base}{i + 1}\"\n",
        "\n",
        "    # Check if the destination folder already exists\n",
        "    if not os.path.exists(destination_folder):\n",
        "        os.makedirs(destination_folder)\n",
        "\n",
        "    # Move images to the destination folder\n",
        "    for j in range(image_count + 1, image_count + 126):\n",
        "        image = f\"{j}.jpg\"\n",
        "        source_path = os.path.join(source_folder, image)\n",
        "        destination_path = os.path.join(destination_folder, image)\n",
        "\n",
        "        # Check if the source image file exists\n",
        "        if os.path.exists(source_path):\n",
        "            shutil.move(source_path, destination_path)\n",
        "\n",
        "    # Update the counters\n",
        "    image_count += num_images_per_folder\n",
        "\n",
        "print(\"All images have been moved.\")\n",
        "remaining_images = [f for f in os.listdir(source_folder) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "print(len(remaining_images))\n"
      ],
      "metadata": {
        "id": "zWbhlQMaVoAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "gc.collect()\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "a39vVRNceD6A"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Organize data and directories\n",
        "original_dir = '/content/dataset/720p/'  # Directory containing original 1080p images\n",
        "scaled_dir = '/content/dataset/360s/'  # Directory containing scaled images\n",
        "\n",
        "# Step 2: Load and preprocess the images\n",
        "original_images = []\n",
        "scaled_images = []\n",
        "\n",
        "for filename in os.listdir(original_dir):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):  # Adjust file extensions if needed\n",
        "        original_path = os.path.join(original_dir, filename)\n",
        "        scaled_path = os.path.join(scaled_dir, filename)\n",
        "\n",
        "        original_image = cv2.imread(original_path)\n",
        "        scaled_image = cv2.imread(scaled_path)\n",
        "\n",
        "        # Preprocess the images (resize and normalize)\n",
        "        original_image = cv2.resize(original_image, (640, 360))\n",
        "        scaled_image = cv2.resize(scaled_image, (640, 360))\n",
        "\n",
        "        original_images.append(original_image)\n",
        "        scaled_images.append(scaled_image)\n",
        "\n",
        "original_images = np.array(original_images)\n",
        "scaled_images = np.array(scaled_images)\n",
        "\n",
        "original_images = original_images.transpose((0, 3, 1, 2))  # Transpose from (num_samples, 640, 360, 3) to (num_samples, 3, 640, 360)\n",
        "scaled_images = scaled_images.transpose((0, 3, 1, 2))  # Transpose from (num_samples, 640, 360, 3) to (num_samples, 3, 640, 360)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Convert the input data to the correct shape and move to GPU\n",
        "original_images = torch.from_numpy(original_images).to(device).float()\n",
        "scaled_images = torch.from_numpy(scaled_images).to(device).float()\n",
        "\n",
        "# Define the U-Net model architecture\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.encoder2 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.encoder3 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.encoder4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder5 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        )\n",
        "        self.decoder6 = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        )\n",
        "        self.decoder7 = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        )\n",
        "        self.decoder8 = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 3, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = self.encoder1(x)\n",
        "        x2 = self.encoder2(x1)\n",
        "        x3 = self.encoder3(x2)\n",
        "        x4 = self.encoder4(x3)\n",
        "\n",
        "        # Decoder\n",
        "        d5 = self.decoder5(x4)\n",
        "        d4 = self.decoder6(torch.cat([d5, x3], dim=1))\n",
        "        d3 = self.decoder7(torch.cat([d4, x2], dim=1))\n",
        "        d2 = self.decoder8(torch.cat([d3, x1], dim=1))\n",
        "\n",
        "        return d2\n",
        "\n",
        "# Step 2: Load the previous model checkpoint\n",
        "#checkpoint_path = '/content/checkpoints/model_epoch_6.pth'  # Path to the previous model checkpoint file\n",
        "\n",
        "# Create an instance of the U-Net model\n",
        "model = UNet()\n",
        "\n",
        "# Load the model checkpoint\n",
        "# checkpoint = torch.load(checkpoint_path)\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Move the model to GPU if available\n",
        "model.to(device)\n",
        "\n",
        "# Print the model architecture\n",
        "#print(model)\n",
        "\n",
        "# Step 5: Compile and train the model\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, verbose=True)\n",
        "# Create lists to store training and validation losses\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Define a custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, inputs, targets, transform=None):\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_image = self.inputs[idx]\n",
        "        target_image = self.targets[idx]\n",
        "\n",
        "        # Convert tensors to PIL images\n",
        "        input_image = ToPILImage()(input_image)\n",
        "        target_image = ToPILImage()(target_image)\n",
        "\n",
        "        if self.transform:\n",
        "            input_image = self.transform(input_image)\n",
        "            target_image = self.transform(target_image)\n",
        "\n",
        "        return input_image, target_image\n",
        "\n",
        "\n",
        "# Create dataloaders for training and validation\n",
        "train_dataset = CustomDataset(scaled_images, original_images, transform=ToTensor())\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "val_dataset = CustomDataset(scaled_images, original_images, transform=ToTensor())  # Replace with your validation dataset\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(9):\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_dataloader:\n",
        "        inputs = inputs.to(device).float()  # Convert input data to float\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {running_loss}\")\n",
        "\n",
        "    # Calculate training loss\n",
        "    train_loss = running_loss / len(train_dataloader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Calculate validation loss\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        for val_inputs, val_targets in val_dataloader:\n",
        "            val_inputs = val_inputs.to(device).float()\n",
        "            val_targets = val_targets.to(device)\n",
        "\n",
        "            val_outputs = model(val_inputs)\n",
        "            loss = criterion(val_outputs, val_targets)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "        val_loss /= len(val_dataloader)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "    # Adjust learning rate based on the validation loss\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Check for overfitting\n",
        "    if epoch > 0:\n",
        "        if val_losses[epoch] > val_losses[epoch-1]:\n",
        "            print(\"Model is starting to overfit. Stopping training.\")\n",
        "            break\n",
        "\n",
        "    # Save checkpoint after every epoch\n",
        "    checkpoint_path = f'/content/checkpoints/model_epoch_{epoch+1}.pth'\n",
        "    torch.save({\n",
        "        'epoch': epoch+1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': running_loss\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at: {checkpoint_path}\")\n",
        "\n",
        "# Plot training and validation losses\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Step 6: Save the trained model\n",
        "torch.save(model.state_dict(), '/content/model.pth')\n",
        "print(\"Model saved.\")\n",
        "\n",
        "\n",
        "# Perform memory cleanup\n",
        "gc.collect()\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "ahGxAQN1lrUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "\n",
        "# Load the trained model\n",
        "model = UNet()\n",
        "model.load_state_dict(torch.load('/content/model.pth'))\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Load and preprocess the input image\n",
        "input_image = cv2.imread('/content/dataset/360s/045.jpg')\n",
        "input_image = cv2.resize(input_image, (640, 360))\n",
        "input_tensor = ToTensor()(input_image).unsqueeze(0).to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    # Forward pass\n",
        "    output_tensor = model(input_tensor)\n",
        "\n",
        "# Convert the output tensor to a numpy array\n",
        "output_image = output_tensor.squeeze(0).cpu().numpy()\n",
        "\n",
        "# Convert the output tensor to a numpy array\n",
        "output_image = output_tensor.squeeze(0).cpu().numpy()\n",
        "\n",
        "# Transpose the output array to match the shape (height, width, channels)\n",
        "output_image = np.transpose(output_image, (1, 2, 0))\n",
        "\n",
        "# Scale the pixel values from [0, 1] to [0, 255]\n",
        "output_image = (output_image * 255).astype(np.uint8)\n",
        "\n",
        "# Convert the output array to a PIL image\n",
        "output_image = Image.fromarray(output_image)\n",
        "\n",
        "# Save the predicted image\n",
        "output_image.save('/content/predicted/predicted_image.jpg')"
      ],
      "metadata": {
        "id": "wgp9HbU2VCHg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load the original and predicted images\n",
        "original_image = Image.open('/content/dataset/720p/045.jpg')\n",
        "predicted_image = Image.open('/content/predicted/predicted_image.jpg')\n",
        "\n",
        "# Convert images to numpy arrays\n",
        "original_array = np.array(original_image)\n",
        "predicted_array = np.array(predicted_image)\n",
        "\n",
        "# Calculate the pixel-wise difference\n",
        "diff_array = np.abs(original_array - predicted_array)\n",
        "\n",
        "# Calculate the number of mismatched pixels\n",
        "num_mismatched_pixels = np.sum(diff_array > 0)\n",
        "\n",
        "# Calculate the total number of pixels\n",
        "total_pixels = original_array.size\n",
        "\n",
        "# Calculate the accuracy as the percentage of matching pixels\n",
        "accuracy = ((total_pixels - num_mismatched_pixels) / total_pixels) * 100\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvlEYVv9VFMP",
        "outputId": "7ae9070a-7e7e-48e8-bac0-8fc67e4fdfd9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 9.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf temp_scaled\n",
        "!mkdir temp_scaled\n",
        "!cp /content/checkpoints/model_epoch_6.pth /content/temp_model/model_epoch_6.pth"
      ],
      "metadata": {
        "id": "0x2xymyb1Q5r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}